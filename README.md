# LLM Multi-Provider Framework

Autonomous AI orchestration system with support for multiple LLM providers including Claude and Ollama, featuring continuous agent execution, GitHub Copilot integration, and automated PR management.

## Features

- **Multi-Provider Support**: Seamlessly switch between different LLM providers (Claude, Ollama)
- **Agent System**: Pre-built specialized agents for different tasks:
  - Research Agent: Information gathering and analysis
  - Coding Agent: Software development assistance
  - Writing Agent: Content creation and editing
  - Code Review Agent: ‚ú® NEW - Automated PR code review
- **Continuous Execution**: Agents can run continuously, processing task queues autonomously
- **GitHub Integration**: 
  - Agents can send prompts and results to GitHub Copilot via issues/comments
  - ‚ú® NEW - **PR Automation**: Auto-review, validate, create, and merge pull requests
  - ‚ú® NEW - **GitHub CLI Fallback**: Works with `gh` CLI when API token unavailable
- **Extensible Architecture**: Easy to add custom providers and agents
- **Provider Abstraction**: Unified interface across different LLM backends

## ‚ú® NEW: GitHub PR Automation

Automate your pull request workflow with AI-powered code review:

- **Auto-Review**: Automatically review PRs when opened or updated
- **Validation**: Check if PRs are ready to merge (checks, approvals, conflicts)
- **Auto-Merge**: Merge PRs automatically when all conditions are met
- **PR Creation**: Create pull requests programmatically
- **GitHub CLI Support**: Use `gh` CLI as fallback when API token isn't available

See [PR Automation Documentation](docs/PR_AUTOMATION.md) for detailed guide.

### Quick Start with PR Automation

```bash
# Option 1: Using GitHub API (with token)
export GITHUB_TOKEN=your_token
export ANTHROPIC_API_KEY=your_key  # or use Ollama

# Option 2: Using GitHub CLI (no token needed)
# Install and authenticate gh CLI first:
# gh auth login

# Create a PR
python -m llm_framework.scripts.create_pr \
  --title "Add feature" --head feature-branch --base main

# Review a PR
python -m llm_framework.scripts.auto_review_pr --pr-number 123

# Auto-merge when ready (add 'auto-merge' label to PR)
```

### GitHub CLI Setup (Optional)

The framework can use GitHub CLI (`gh`) as a fallback for GitHub operations:

```bash
# Install GitHub CLI (if not already installed)
# On macOS
brew install gh

# On Ubuntu/Debian
sudo apt install gh

# On other systems, see: https://cli.github.com/

# Authenticate
gh auth login

# Verify authentication
gh auth status
```

When `gh` CLI is authenticated, the framework will automatically use it as a fallback
when the `GITHUB_TOKEN` environment variable is not set. You can also explicitly prefer
CLI over API by passing `prefer_cli=True` to `GitHubIntegration`.

## Installation

1. Clone the repository:
```bash
git clone https://github.com/Scarmonit/LLM.git
cd LLM
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

## ü§ñ Enhanced GitHub Copilot Integration

This repository is configured with advanced GitHub Copilot features to make the AI assistant smarter and more context-aware:

### What Makes Copilot Smarter Here?

‚úÖ **Custom Instructions** - Repository-specific guidance in `.github/copilot-instructions.md`  
‚úÖ **Custom Agent** - Specialized "LLM Framework Expert" agent for domain-specific help  
‚úÖ **MCP Servers** - Additional tools via Model Context Protocol (browser automation, GitHub operations)  
‚úÖ **Optimized DevContainer** - Pre-configured development environment with all tools ready  

### Quick Benefits

- **Context-Aware Suggestions**: Copilot understands the framework architecture and patterns
- **Avoid Past Mistakes**: Knows about documented pitfalls and anti-patterns  
- **Enhanced Tools**: Can use browser automation and advanced Git operations via MCP
- **Specialized Help**: Ask `@llm-framework-expert` for framework-specific guidance
- **Instant Setup**: Open in DevContainer and everything is ready

### Getting Started

#### Option 1: GitHub Codespaces (Recommended)
```bash
# 1. Click "Code" ‚Üí "Codespaces" ‚Üí "Create codespace"
# 2. Wait for environment to initialize
# 3. Start coding with enhanced Copilot!
```

#### Option 2: VS Code DevContainer
```bash
# 1. Install "Dev Containers" extension
# 2. Open repository in VS Code
# 3. Click "Reopen in Container" when prompted
# 4. Copilot is now enhanced with custom features
```

#### Option 3: Local Development
```bash
git clone https://github.com/Scarmonit/LLM.git
cd LLM
pip install -r requirements.txt
# Copilot custom instructions work, but MCP servers require DevContainer
```

### Practical Examples: Working with Copilot

#### Creating Copilot-Friendly Issues

**‚úÖ Good - Structured with clear acceptance criteria:**
```markdown
Title: [BUG] OllamaProvider timeout on responses >1000 tokens

Problem: Timeout occurs when processing large responses
Acceptance Criteria:
- Increase timeout to 120s for large requests
- Add exponential backoff retry logic
- Tests verify timeout handling
Affected Files: src/llm_framework/providers/ollama_provider.py
```

**‚ùå Bad - Vague and unscoped:**
```markdown
Title: Fix the timeouts
Description: The provider times out sometimes
```

**Why the difference matters**: Well-scoped issues enable Copilot to deliver precise solutions faster with fewer iterations.

#### Using Issue Templates

This repository provides structured templates in `.github/ISSUE_TEMPLATE/`:
- **Bug Fix** - Problem statements, reproduction steps, acceptance criteria
- **Feature Request** - User stories, component impact, design considerations  
- **Refactoring** - Current problems, improvements, backward compatibility
- **Documentation** - Target audience, examples needed, topics to cover

**To create an issue:**
1. Go to "Issues" ‚Üí "New issue"
2. Select appropriate template
3. Fill in all sections
4. Assign to @copilot for AI assistance

#### Delegating Tasks to Copilot

```markdown
<!-- Assign issue to @copilot with clear context -->
@copilot Please implement the retry logic described in the acceptance criteria.

Context:
- See OllamaProvider.generate() for similar implementation
- Follow patterns in docs/ARCHITECTURE.md
- Run tests: pytest tests/test_claude_provider.py -v

Expected outcome: All acceptance criteria met, tests passing, no mock data
```

### Using the Custom Copilot Agent

Ask domain-specific questions:
```
@llm-framework-expert How do I add a new LLM provider?
@llm-framework-expert What's the best temperature for a research agent?
@llm-framework-expert My agents aren't starting. How do I debug?
```

### MCP Servers Available

- **playwright**: Browser automation and testing
- **github-mcp-server**: Enhanced GitHub repository operations

**Learn More**: 
- [Contributing Guide](CONTRIBUTING.md) - Task scoping guidelines with good/bad examples
- [Copilot Instructions](.github/copilot-instructions.md) - Complete Copilot guidance
- [Issue Templates](.github/ISSUE_TEMPLATE/) - Structured templates for clear tasks
- [Copilot Enhancement Guide](docs/COPILOT_GUIDE.md) - Advanced features and best practices

## Development Environment

Pre-configured DevContainer includes:
- Python 3.11 with all dependencies
- Node.js 20 for MCP servers
- GitHub CLI for Git operations
- GitHub Copilot with custom configuration
- Automatic setup on container creation

## Quick Start

### Using Claude Provider

Set your Anthropic API key:
```bash
export ANTHROPIC_API_KEY=your_api_key_here
```

### Using Ollama Provider

Install and run Ollama:
```bash
# Install Ollama from https://ollama.ai
# Pull a model
ollama pull llama2

# Ollama server runs on http://localhost:11434 by default
```

### Running a Single Agent

```bash
python example.py
```

### Running Continuous Agents

Run agents continuously with automatic task processing:

```bash
# Run all agents continuously
python run_continuous.py --agent all --interval 60

# Run specific agent
python run_continuous.py --agent research --interval 30

# With GitHub integration
export GITHUB_REPO_OWNER=your-username
export GITHUB_REPO_NAME=your-repo
export GITHUB_TOKEN=your-github-token
python run_continuous.py --agent all --issue-number 123
```

Options:
- `--agent`: Choose which agent to run (`research`, `coding`, `writing`, or `all`)
- `--interval`: Time between task checks in seconds (default: 60)
- `--max-iterations`: Maximum iterations before stopping (default: unlimited)
- `--repo-owner`: GitHub repository owner
- `--repo-name`: GitHub repository name
- `--issue-number`: GitHub issue number for threaded conversation

## Usage

### Basic Usage

```python
from llm_framework.orchestrator import AgentOrchestrator

# Create orchestrator
orchestrator = AgentOrchestrator()

# Set up providers
orchestrator.setup_default_providers()

# Set up agents
orchestrator.setup_default_agents()

# Use an agent
research_agent = orchestrator.get_agent("research")
result = research_agent.execute("What are the latest AI trends?")
print(result)
```

### Continuous Agent Execution

```python
from llm_framework.orchestrator import AgentOrchestrator
from llm_framework.continuous_agent import ContinuousAgent

# Set up orchestrator
orchestrator = AgentOrchestrator()
orchestrator.setup_default_providers()
orchestrator.setup_default_agents()

# Get an agent
agent = orchestrator.get_agent("research")

# Create continuous agent
continuous = ContinuousAgent(agent, interval=60)

# Add tasks to the queue
continuous.add_task("Research quantum computing")
continuous.add_task("Analyze market trends in AI")

# Start continuous execution
continuous.start()

# Check status
status = continuous.get_status()
print(f"Running: {status['is_running']}, Iterations: {status['iteration_count']}")

# Stop when done
continuous.stop()
```

### GitHub Integration

```python
from llm_framework.github_integration import GitHubIntegration, AgentGitHubBridge
from llm_framework.continuous_agent import ContinuousAgent

# Set up GitHub integration
github = GitHubIntegration("your-username", "your-repo")
bridge = AgentGitHubBridge(github, issue_number=123)

# Set up continuous agent with GitHub callback
continuous = ContinuousAgent(agent, interval=60)
continuous.on_result_callback = bridge  # Results sent to GitHub

# Add tasks and start
continuous.add_task("Research AI ethics")
continuous.start()
```

The agent will automatically send task results to the specified GitHub issue, mentioning @copilot for review.

## Creating Custom Agents

```python
from llm_framework.core.agent import Agent, AgentConfig

config = AgentConfig(
    name="Custom Agent",
    description="My custom agent",
    system_prompt="You are a specialized assistant...",
    temperature=0.7
)

agent = Agent(config, provider)
result = agent.execute("Your task here")
```

## Testing

Run tests with pytest:
```bash
pytest tests/
```

## Project Structure

```
LLM/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ llm_framework/
‚îÇ       ‚îú‚îÄ‚îÄ core/                  # Core framework components
‚îÇ       ‚îú‚îÄ‚îÄ providers/             # LLM provider implementations
‚îÇ       ‚îú‚îÄ‚îÄ agents/                # Pre-built agent implementations
‚îÇ       ‚îú‚îÄ‚îÄ orchestrator.py        # Agent orchestration
‚îÇ       ‚îú‚îÄ‚îÄ continuous_agent.py    # Continuous execution support
‚îÇ       ‚îî‚îÄ‚îÄ github_integration.py  # GitHub Copilot integration
‚îú‚îÄ‚îÄ tests/                         # Unit tests
‚îú‚îÄ‚îÄ example.py                     # Basic example
‚îú‚îÄ‚îÄ run_continuous.py              # Continuous agent runner
‚îî‚îÄ‚îÄ requirements.txt               # Python dependencies
```

## Environment Variables

- `ANTHROPIC_API_KEY`: API key for Claude provider
- `OLLAMA_BASE_URL`: Ollama server URL (default: http://localhost:11434)
- `GITHUB_TOKEN`: GitHub personal access token for integration
- `GITHUB_REPO_OWNER`: Repository owner for GitHub integration
- `GITHUB_REPO_NAME`: Repository name for GitHub integration

## Always-Running Deployment

To ensure agents are **always running** in production, see the [Deployment Guide](deployment/DEPLOYMENT.md) for multiple options:

### Quick Deploy Options

**Docker (Recommended):**
```bash
cp .env.example .env  # Configure your API keys
docker-compose up -d
```

**Linux systemd:**
```bash
sudo deployment/systemd/install.sh
sudo systemctl enable llm-agents
sudo systemctl start llm-agents
```

**PM2 (Cross-platform):**
```bash
pm2 start deployment/pm2/ecosystem.config.json
pm2 save && pm2 startup
```

All options include:
- ‚úÖ Automatic restart on failure
- ‚úÖ Auto-start on system boot
- ‚úÖ Log management
- ‚úÖ Process monitoring

See [deployment/DEPLOYMENT.md](deployment/DEPLOYMENT.md) for complete instructions.

## License

MIT License
