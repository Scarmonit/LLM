"""Ollama LLM provider.\n\n"""\n\nfrom typing import Any, Dict\nimport httpx\nfrom ..exceptions import ProviderUnavailable\n\n\nclass OllamaProvider:\n    """Ollama provider that connects to local Ollama instance."""\n\n    def __init__(self, base_url: str = "http://localhost:11434", model: str = "llama3", **kwargs):\n        self.base_url = base_url\n        self.model = model\n        self.temperature = kwargs.get("temperature", 0.7)\n        self.timeout = kwargs.get("timeout", 3.0)\n\n    def _check_availability(self) -> bool:\n        """Check if Ollama endpoint is reachable."""\n        try:\n            response = httpx.get(f"{self.base_url}/api/tags", timeout=2.0)\n            return response.status_code == 200\n        except (httpx.RequestError, httpx.TimeoutException):\n            return False\n\n    def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:\n        """Generate a response using Ollama.\n\n        Args:\n            prompt: The input prompt\n            **kwargs: Additional parameters\n\n        Returns:\n            Dict with provider, model, and content fields\n\n        Raises:\n            ProviderUnavailable: If Ollama endpoint is not reachable\n        """\n        if not self._check_availability():\n            raise ProviderUnavailable(\n                f"Ollama endpoint at {self.base_url} is not reachable. "\n                "Please ensure Ollama is running."\n            )\n\n        try:\n            payload = {\n                "model": self.model,\n                "prompt": prompt,\n                "stream": False,\n                "options": {\n                    "temperature": kwargs.get("temperature", self.temperature),\n                },\n            }\n\n            response = httpx.post(\n                f"{self.base_url}/api/generate", json=payload, timeout=self.timeout\n            )\n            response.raise_for_status()\n\n            data = response.json()\n\n            return {\n                "provider": "ollama",\n                "model": data.get("model", self.model),\n                "content": data.get("response", ""),\n                "prompt": prompt,\n            }\n        except httpx.HTTPError as e:\n            raise ProviderUnavailable(f"Ollama request failed: {str(e)}") from e\n\n    async def agenerate(self, prompt: str, **kwargs) -> Dict[str, Any]:\n        """Async version of generate.\n\n        Args:\n            prompt: The input prompt\n            **kwargs: Additional parameters\n\n        Returns:\n            Dict with provider, model, and content fields\n\n        Raises:\n            ProviderUnavailable: If Ollama endpoint is not reachable\n        """\n        if not self._check_availability():\n            raise ProviderUnavailable(\n                f"Ollama endpoint at {self.base_url} is not reachable. "\n                "Please ensure Ollama is running."\n            )\n\n        try:\n            payload = {\n                "model": self.model,\n                "prompt": prompt,\n                "stream": False,\n                "options": {\n                    "temperature": kwargs.get("temperature", self.temperature),\n                },\n            }\n\n            async with httpx.AsyncClient() as client:\n                response = await client.post(\n                    f"{self.base_url}/api/generate", json=payload, timeout=self.timeout\n                )\n                response.raise_for_status()\n\n                data = response.json()\n\n                return {\n                    "provider": "ollama",\n                    "model": data.get("model", self.model),\n                    "content": data.get("response", ""),\n                    "prompt": prompt,\n                }\n        except httpx.HTTPError as e:\n            raise ProviderUnavailable(f"Ollama request failed: {str(e)}") from e